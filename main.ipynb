{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c9b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube RAG QA Bot - Complete Implementation with LangSmith\n",
    "# ===========================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import whisper\n",
    "import gradio as gr\n",
    "import nest_asyncio\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from yt_dlp import YoutubeDL \n",
    "#import librosa #not really useful\n",
    "\n",
    "nest_asyncio.apply()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# LangChain Core\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.agents import initialize_agent, AgentType, Tool\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from pydantic import BaseModel, PrivateAttr\n",
    "\n",
    "# Mistral Integration\n",
    "from mistralai import Mistral, UserMessage\n",
    "\n",
    "# LangSmith Integration\n",
    "from langsmith import Client\n",
    "from langchain.callbacks import LangChainTracer\n",
    "from langchain.schema import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac6d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LangSmith tracing enabled\n"
     ]
    }
   ],
   "source": [
    "# ==================== Configuration ====================\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_PROJECT = os.getenv(\"LANGCHAIN_PROJECT\", \"thisonecanbefinal3\")\n",
    "\n",
    "if not MISTRAL_API_KEY:\n",
    "    raise ValueError(\"❌ MISTRAL_API_KEY not found in environment variables\")\n",
    "\n",
    "# LangSmith Configuration\n",
    "if LANGCHAIN_API_KEY:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = LANGCHAIN_PROJECT\n",
    "    print(\"✅ LangSmith tracing enabled\")\n",
    "else:\n",
    "    print(\"⚠️ LANGCHAIN_API_KEY not found - LangSmith tracing disabled\")\n",
    "\n",
    "# Directory Configuration\n",
    "BASE_DIR = \"youtube_rag_project\"\n",
    "AUDIO_DIR = os.path.join(BASE_DIR, \"audio\")\n",
    "TRANSCRIPTS_DIR = os.path.join(BASE_DIR, \"transcripts\")\n",
    "CHROMA_DIR = os.path.join(BASE_DIR, \"chroma_db\")\n",
    "TTS_DIR = os.path.join(BASE_DIR, \"tts\")\n",
    "\n",
    "# Model Configuration\n",
    "WHISPER_MODEL_SIZE = \"base\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "VIDEO_LIMIT = 30\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [AUDIO_DIR, TRANSCRIPTS_DIR, TTS_DIR, CHROMA_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e924872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LangSmith Integration ====================\n",
    "class LangSmithTracker:\n",
    "    \"\"\"LangSmith integration for monitoring and evaluation\"\"\"\n",
    "    \n",
    "def __init__(self):\n",
    "    self.client = None\n",
    "    if LANGCHAIN_API_KEY:\n",
    "        try:\n",
    "            self.client = Client(api_key=LANGCHAIN_API_KEY)\n",
    "            from langsmith.schemas import TracerSessionCreate\n",
    "            \n",
    "            session_name = f\"youtube-rag-session-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "            session = self.client.create_session(\n",
    "                TracerSessionCreate(name=session_name)\n",
    "            )\n",
    "            self.session_id = session.id\n",
    "            \n",
    "            logging.info(f\"✅ LangSmith client initialized, session: {session_name}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"LangSmith initialization error: {e}\")\n",
    "            self.client = None\n",
    "    \n",
    "def log_interaction(self, question: str, answer: str, sources: List[str] = None):\n",
    "    \"\"\"Log user interactions for evaluation\"\"\"\n",
    "    if not self.client:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        self.client.create_run(\n",
    "            name=\"youtube_rag_query\",\n",
    "            run_type=\"chain\",\n",
    "            inputs={\"question\": question},\n",
    "            outputs={\"answer\": answer, \"sources\": sources or []},\n",
    "            project_name=LANGCHAIN_PROJECT,\n",
    "            session_id=self.session_id,\n",
    "            start_time=datetime.now(),\n",
    "            end_time=datetime.now()\n",
    "        )\n",
    "        logging.info(\"✅ Interaction logged to LangSmith\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LangSmith logging error: {e}\")\n",
    "        \n",
    "    \n",
    "    def create_evaluation_dataset(self, qa_pairs: List[Dict]):\n",
    "        \"\"\"Create evaluation dataset in LangSmith\"\"\"\n",
    "        if not self.client:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            dataset_name = f\"youtube-rag-eval-{datetime.now().strftime('%Y%m%d')}\"\n",
    "            dataset = self.client.create_dataset(\n",
    "                dataset_name=dataset_name,\n",
    "                description=\"YouTube RAG Bot evaluation dataset\"\n",
    "            )\n",
    "            \n",
    "            for qa_pair in qa_pairs:\n",
    "                self.client.create_example(\n",
    "                    dataset_id=dataset.id,\n",
    "                    inputs={\"question\": qa_pair[\"question\"]},\n",
    "                    outputs={\"expected_answer\": qa_pair[\"answer\"]}\n",
    "                )\n",
    "            \n",
    "            logging.info(f\"✅ Created evaluation dataset: {dataset_name}\")\n",
    "            return dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Dataset creation error: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed5a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Mistral LLM Wrapper ====================\n",
    "class MistralLLM(LLM, BaseModel):\n",
    "    \"\"\"Custom Mistral LLM wrapper for LangChain\"\"\"\n",
    "    api_key: str\n",
    "    model: str = \"mistral-large-latest\"\n",
    "    temperature: float = 0.0\n",
    "    max_tokens: int = 1000\n",
    "    _mistral_client: Any = PrivateAttr()\n",
    "\n",
    "    def __init__(self, **data: Any):\n",
    "        super().__init__(**data)\n",
    "        self._mistral_client = Mistral(api_key=self.api_key)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"mistral\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        try:\n",
    "            messages = [UserMessage(content=prompt)]\n",
    "            response = self._mistral_client.chat.complete(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Mistral API error: {e}\")\n",
    "            return f\"❌ Error: Unable to process request\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4540fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Data Processing Pipeline ====================\n",
    "class YouTubeDataProcessor:\n",
    "    \"\"\"Handles YouTube video downloading and transcription\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.whisper_model = None\n",
    "    \n",
    "    def download_videos(self, channel_url: str, limit: int = VIDEO_LIMIT) -> bool:\n",
    "        \"\"\"Download YouTube videos as audio files\"\"\"\n",
    "        existing_files = glob.glob(os.path.join(AUDIO_DIR, \"*.mp3\"))\n",
    "        \n",
    "        if len(existing_files) >= limit:\n",
    "            logging.info(f\"✅ Found {len(existing_files)} audio files, skipping download\")\n",
    "            return True\n",
    "\n",
    "        logging.info(f\"📥 Downloading {limit} videos from channel...\")\n",
    "        \n",
    "        ydl_opts = {\n",
    "            \"format\": \"bestaudio/best\",\n",
    "            \"postprocessors\": [{\n",
    "                \"key\": \"FFmpegExtractAudio\",\n",
    "                \"preferredcodec\": \"mp3\",\n",
    "                \"preferredquality\": \"128\",\n",
    "            }],\n",
    "            \"playlistend\": limit,\n",
    "            \"outtmpl\": os.path.join(AUDIO_DIR, \"%(title)s.%(ext)s\"),\n",
    "            \"ignoreerrors\": True,\n",
    "            \"quiet\": True,\n",
    "            \"no_warnings\": True\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with YoutubeDL(ydl_opts) as ydl:\n",
    "                ydl.download([channel_url])\n",
    "            logging.info(\"✅ Download completed\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Download error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def transcribe_audio_files(self) -> Dict[str, str]:\n",
    "        \"\"\"Transcribe all audio files to text\"\"\"\n",
    "        if self.whisper_model is None:\n",
    "            logging.info(\"🧠 Loading Whisper model...\")\n",
    "            self.whisper_model = whisper.load_model(WHISPER_MODEL_SIZE)\n",
    "        \n",
    "        transcripts = {}\n",
    "        audio_files = glob.glob(os.path.join(AUDIO_DIR, \"*.mp3\"))\n",
    "        \n",
    "        for audio_file in audio_files:\n",
    "            title = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "            transcript_path = os.path.join(TRANSCRIPTS_DIR, f\"{title}.txt\")\n",
    "            \n",
    "            # Check if transcript already exists\n",
    "            if os.path.exists(transcript_path):\n",
    "                with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    transcripts[title] = f.read()\n",
    "                continue\n",
    "            \n",
    "            logging.info(f\"🎙️ Transcribing: {title[:50]}...\")\n",
    "            try:\n",
    "                result = self.whisper_model.transcribe(audio_file)\n",
    "                transcript_text = result[\"text\"].strip()\n",
    "                transcripts[title] = transcript_text\n",
    "                \n",
    "                # Save transcript\n",
    "                with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(transcript_text)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Transcription error for {title}: {e}\")\n",
    "        \n",
    "        logging.info(f\"✅ Processed {len(transcripts)} transcripts\")\n",
    "        return transcripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "951bde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== RAG Vector Store ====================\n",
    "class RAGVectorStore:\n",
    "    \"\"\"Handles vector database creation and management\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL,\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        self.vectorstore = None\n",
    "    \n",
    "    def create_or_load_vectorstore(self, transcripts: Dict[str, str]) -> Chroma:\n",
    "        \"\"\"Create or load existing vector database\"\"\"\n",
    "        \n",
    "        # Check if vectorstore exists\n",
    "        if os.path.exists(os.path.join(CHROMA_DIR, \"chroma.sqlite3\")):\n",
    "            logging.info(\"✅ Loading existing vectorstore...\")\n",
    "            self.vectorstore = Chroma(\n",
    "                persist_directory=CHROMA_DIR, \n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "            return self.vectorstore\n",
    "        \n",
    "        logging.info(\"📚 Creating new vectorstore...\")\n",
    "        \n",
    "        # Text splitting and chunking\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "        )\n",
    "        \n",
    "        documents = []\n",
    "        for title, transcript in transcripts.items():\n",
    "            if not transcript.strip():\n",
    "                continue\n",
    "                \n",
    "            chunks = text_splitter.split_text(transcript)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                doc = Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"source\": title,\n",
    "                        \"chunk_id\": i,\n",
    "                        \"total_chunks\": len(chunks)\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        \n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents to vectorize\")\n",
    "        \n",
    "        # Create vectorstore\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=CHROMA_DIR\n",
    "        )\n",
    "        \n",
    "        self.vectorstore.persist()\n",
    "        logging.info(f\"✅ Vectorstore created with {len(documents)} chunks\")\n",
    "        return self.vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f029dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Agent Tools ====================\n",
    "def create_vector_search_tool(vectorstore: Chroma) -> Tool:\n",
    "    \"\"\"Create vector search tool for the agent\"\"\"\n",
    "    \n",
    "    def vector_search(query: str) -> str:\n",
    "        \"\"\"Search through YouTube video transcripts\"\"\"\n",
    "        try:\n",
    "            retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={\"k\": 3}\n",
    "            )\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "            \n",
    "            if not docs:\n",
    "                return \"❌ No relevant information found in the video transcripts.\"\n",
    "            \n",
    "            # Format results\n",
    "            results = []\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "                content = doc.page_content[:400]  # Limit content\n",
    "                results.append(f\"**Source {i}: {source}**\\n{content}...\")\n",
    "            \n",
    "            return \"\\n\\n\".join(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Vector search error: {e}\")\n",
    "            return f\"❌ Search error: {str(e)}\"\n",
    "    \n",
    "    return Tool(\n",
    "        name=\"vector_search\",\n",
    "        description=\"Search through YouTube video transcripts for relevant information. Input should be a search query.\",\n",
    "        func=vector_search\n",
    "    )\n",
    "\n",
    "def create_transcription_tool() -> Tool:\n",
    "    \"\"\"Create audio transcription tool\"\"\"\n",
    "    \n",
    "    def transcribe_audio(audio_path: str) -> str:\n",
    "        \"\"\"Transcribe audio file to text\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(audio_path):\n",
    "                return \"❌ Audio file not found\"\n",
    "            \n",
    "            model = whisper.load_model(WHISPER_MODEL_SIZE)\n",
    "            result = model.transcribe(audio_path)\n",
    "            return f\"🎙️ Transcription: {result['text']}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Transcription error: {e}\")\n",
    "            return f\"❌ Transcription failed: {str(e)}\"\n",
    "    \n",
    "    return Tool(\n",
    "        name=\"transcribe_audio\",\n",
    "        description=\"Transcribe audio input to text. Input should be audio file path.\",\n",
    "        func=transcribe_audio\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b55bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Main RAG Agent ====================\n",
    "class YouTubeRAGAgent:\n",
    "    \"\"\"Main RAG agent with tools, memory, and LangSmith tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, mistral_api_key: str, vectorstore: Chroma):\n",
    "        # Initialize LLM\n",
    "        self.llm = MistralLLM(api_key=mistral_api_key)\n",
    "        \n",
    "        # Initialize LangSmith tracker\n",
    "        self.langsmith_tracker = LangSmithTracker()\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = ConversationBufferWindowMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            k=6  # Keep last 3 exchanges\n",
    "        )\n",
    "        \n",
    "        # Initialize tools\n",
    "        self.tools = [\n",
    "            create_vector_search_tool(vectorstore),\n",
    "            create_transcription_tool()\n",
    "        ]\n",
    "        \n",
    "        # Create callbacks for LangSmith\n",
    "        callbacks = []\n",
    "        if LANGCHAIN_API_KEY:\n",
    "            callbacks.append(LangChainTracer(project_name=LANGCHAIN_PROJECT))\n",
    "        \n",
    "        # Create agent\n",
    "        self.agent = initialize_agent(\n",
    "            tools=self.tools,\n",
    "            llm=self.llm,\n",
    "            agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "            memory=self.memory,\n",
    "            verbose=True, \n",
    "            handle_parsing_errors=True,\n",
    "            max_iterations=6, #increased iterations so to get better reply\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process user question and return answer with metadata\"\"\"\n",
    "        try:\n",
    "            # Enhance prompt for better responses\n",
    "            enhanced_prompt = f\"\"\"\n",
    "            You are a friendly and knowledgeable AI assistant that answers questions based **only** on YouTube video transcripts.\n",
    "\n",
    "            TOOLS:\n",
    "            You have access to the `vector_search` tool to look up relevant information.\n",
    "\n",
    "            RULES:\n",
    "            1.Always use the `vector_search` tool before answering. Never answer from general knowledge.\n",
    "            2.Base your answer **only on the transcript chunks returned** by the tool. If nothing is returned, respond honestly: \"Sorry, I couldn't find any info on that topic in the videos I have.\"\n",
    "            3.Be conversational and clear, like you're explaining to a curious friend.\n",
    "            4.Always include the video title(s) from the source in your response.\n",
    "            5.Never invent video titles or make up facts. If you're unsure, say so.\n",
    "\n",
    "            User Question: {question}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.agent.run(enhanced_prompt)\n",
    "            \n",
    "            # Extract sources from the response (basic implementation)\n",
    "            sources = self._extract_sources(response)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": response,\n",
    "                \"sources\": sources,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Query processing error: {e}\")\n",
    "            error_response = f\"❌ Sorry, I encountered an error processing your question: {str(e)}\"\n",
    "            return {\n",
    "                \"answer\": error_response,\n",
    "                \"sources\": [],\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def _extract_sources(self, response: str) -> List[str]:\n",
    "        \"\"\"Extract source information from response\"\"\"\n",
    "        sources = []\n",
    "        lines = response.split('\\n')\n",
    "        for line in lines:\n",
    "            if \"Source\" in line and \":\" in line:\n",
    "                source = line.split(\":\", 1)[1].strip().replace(\"**\", \"\")\n",
    "                if source and source not in sources:\n",
    "                    sources.append(source)\n",
    "        return sources\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear conversation memory\"\"\"\n",
    "        self.memory.clear()\n",
    "        logging.info(\"🧹 Memory cleared\")\n",
    "    \n",
    "    def get_conversation_history(self) -> List[Dict]:\n",
    "        \"\"\"Get formatted conversation history\"\"\"\n",
    "        history = []\n",
    "        if hasattr(self.memory, 'chat_memory'):\n",
    "            for message in self.memory.chat_memory.messages:\n",
    "                if isinstance(message, HumanMessage):\n",
    "                    history.append({\"role\": \"user\", \"content\": message.content})\n",
    "                elif isinstance(message, AIMessage):\n",
    "                    history.append({\"role\": \"assistant\", \"content\": message.content})\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb8468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Text-to-Speech ====================\n",
    "async def text_to_speech_async(text: str) -> Optional[str]:\n",
    "    \"\"\"Convert text to speech using Edge TTS\"\"\"\n",
    "    try:\n",
    "        import edge_tts\n",
    "        \n",
    "        # Clean text for TTS\n",
    "        clean_text = text.replace(\"**\", \"\").replace(\"*\", \"\")\n",
    "        #chatgpt fix\n",
    "        #if len(clean_text) > 500:\n",
    "        #    clean_text = clean_text[:500] + \"...\"\n",
    "        \n",
    "        output_path = os.path.join(TTS_DIR, \"response.mp3\")\n",
    "        communicate = edge_tts.Communicate(clean_text, voice=\"en-US-JennyNeural\")\n",
    "        await communicate.save(output_path)\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"TTS Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def text_to_speech(text: str) -> Optional[str]:\n",
    "    \"\"\"Synchronous wrapper for TTS\"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    \n",
    "    return loop.run_until_complete(text_to_speech_async(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce85b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Main Processing Pipeline ====================\n",
    "def process_question(text_input: str, audio_input) -> tuple:\n",
    "    \"\"\"Main processing pipeline for questions\"\"\"\n",
    "    question = \"\"\n",
    "    \n",
    "    # Process audio input first\n",
    "    if audio_input is not None:\n",
    "        try:\n",
    "            # Load and process audio\n",
    "            #audio_data, sr = librosa.load(audio_input, sr=16000)\n",
    "            \n",
    "            # Transcribe using Whisper\n",
    "            model = whisper.load_model(WHISPER_MODEL_SIZE)\n",
    "            result = model.transcribe(audio_input)\n",
    "            question = result[\"text\"].strip()\n",
    "            \n",
    "            logging.info(f\"🎙️ Audio transcribed: {question}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Audio processing error: {e}\")\n",
    "            return f\"❌ Audio processing failed: {str(e)}\", None, \"\"\n",
    "    \n",
    "    # Use text input if no audio question\n",
    "    if not question and text_input:\n",
    "        question = text_input.strip()\n",
    "    \n",
    "    # Validate input\n",
    "    if not question:\n",
    "        return \"⚠️ Please provide a question via text or audio.\", None, \"\"\n",
    "    \n",
    "    # Get answer from RAG agent\n",
    "    try:\n",
    "        result = rag_agent.query(question)\n",
    "        answer = result[\"answer\"]\n",
    "        sources = result[\"sources\"]\n",
    "        \n",
    "        # Format sources for display\n",
    "        sources_text = \"\"\n",
    "        if sources:\n",
    "            sources_text = \"\\n\\n**Sources:**\\n\" + \"\\n\".join([f\"• {source}\" for source in sources])\n",
    "        \n",
    "        # Generate audio response\n",
    "        audio_path = text_to_speech(answer)\n",
    "        \n",
    "        return answer + sources_text, audio_path, question\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Processing error: {e}\")\n",
    "        return f\"❌ Error processing question: {str(e)}\", None, question\n",
    "\n",
    "def get_conversation_history():\n",
    "    \"\"\"Get formatted conversation history\"\"\"\n",
    "    try:\n",
    "        history = rag_agent.get_conversation_history()\n",
    "        if not history:\n",
    "            return \"No conversation history yet.\"\n",
    "        \n",
    "        formatted_history = []\n",
    "        for entry in history[-6:]:  # Show last 6 entries\n",
    "            role_icon = \"🧑\" if entry[\"role\"] == \"user\" else \"🤖\"\n",
    "            formatted_history.append(f\"{role_icon} **{entry['role'].title()}**: {entry['content']}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_history)\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving history: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f21d7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Initialize System ====================\n",
    "def initialize_system():\n",
    "    \"\"\"Initialize the complete RAG system\"\"\"\n",
    "    global rag_agent\n",
    "    \n",
    "    logging.info(\"🚀 Initializing YouTube RAG Bot...\")\n",
    "    \n",
    "    # Initialize data processor\n",
    "    processor = YouTubeDataProcessor()\n",
    "    \n",
    "    # Download videos (Life Noggin channel)\n",
    "    channel_url = \"https://www.youtube.com/@lifenoggin/videos\"\n",
    "    processor.download_videos(channel_url, VIDEO_LIMIT)\n",
    "    \n",
    "    # Transcribe videos\n",
    "    transcripts = processor.transcribe_audio_files()\n",
    "    \n",
    "    if not transcripts:\n",
    "        raise ValueError(\"No transcripts found. Please check video download and transcription.\")\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store_manager = RAGVectorStore()\n",
    "    vectorstore = vector_store_manager.create_or_load_vectorstore(transcripts)\n",
    "    \n",
    "    # Initialize RAG agent\n",
    "    rag_agent = YouTubeRAGAgent(MISTRAL_API_KEY, vectorstore)\n",
    "    \n",
    "    logging.info(\"✅ System initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a234fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Initialize Gradio ====================\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create the Gradio web interface with LangSmith monitoring and images\"\"\"\n",
    "\n",
    "    with gr.Blocks(\n",
    "        title=\"🧠 Blocko Bot The open source alternative of knowledge\", \n",
    "        theme=gr.themes.Soft(),\n",
    "        css=\"\"\"\n",
    "        /* Cyan gradient background */\n",
    "        .gradio-container {\n",
    "            background: linear-gradient(135deg, #00c6ff 0%, #0072ff 100%) !important;\n",
    "            min-height: 100vh;\n",
    "            padding: 40px;\n",
    "            width: 100vw;\n",
    "            max-width: 100% ! important;\n",
    "        }        \n",
    "        .gradio-container {\n",
    "        max-width: 1400px !important;\n",
    "        background-color: rgba(255, 255, 255, 0.85);\n",
    "        padding: 20px;\n",
    "        border-radius: 16px;\n",
    "        }\n",
    "        .langsmith-status {\n",
    "            background: linear-gradient(90deg, #10b981, #059669);\n",
    "            color: white;\n",
    "            padding: 10px;\n",
    "            border-radius: 8px;\n",
    "            margin: 10px 0;\n",
    "            text-align: center;\n",
    "        }\n",
    "        \"\"\"\n",
    "    ) as interface:\n",
    "        \n",
    "        # ✅ Banner Image (Local)\n",
    "        gr.Image(value=\"picture/channels4_banner.jpg\", label=None, interactive=False, show_label=False)\n",
    "\n",
    "\n",
    "\n",
    "        langsmith_status = \"🟢 LangSmith Tracking: ON\" if LANGCHAIN_API_KEY else \"🟡 LangSmith Tracking: OFF\"\n",
    "\n",
    "        gr.Markdown(f\"\"\"\n",
    "        ## 🧠 YouTube RAG QA Bot with LangChain Agents\n",
    "\n",
    "        <div class=\"langsmith-status\">\n",
    "        {langsmith_status} | Project: {LANGCHAIN_PROJECT}\n",
    "        </div>\n",
    "\n",
    "        This bot allows you to ask questions about YouTube videos using advanced AI techniques:\n",
    "        - 🤖 **LangChain Agents** with tools & memory\n",
    "        - 📚 **Vector Search** via Chroma\n",
    "        - 🎙️ **Voice Input & Output** (Whisper + Edge TTS)\n",
    "        - 📊 **LangSmith Monitoring**\n",
    "        \"\"\")\n",
    "\n",
    "        with gr.Tab(\"💬 Chat Interface\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    # ✅ Updated Bot Avatar Image Path\n",
    "                    gr.Image(value=\"picture/blocko.png\", label=\"🤖 Hi there! I'm Blocko\", interactive=False)\n",
    "\n",
    "                    text_input = gr.Textbox(\n",
    "                        label=\"💬 Ask your question\",\n",
    "                        placeholder=\"e.g., What happens to your body in space?\",\n",
    "                        lines=3\n",
    "                    )\n",
    "\n",
    "                    audio_input = gr.Audio(\n",
    "                        label=\"🎙️ Or ask with your voice\",\n",
    "                        type=\"filepath\"\n",
    "                    )\n",
    "\n",
    "                    with gr.Row():\n",
    "                        ask_btn = gr.Button(\"🚀 Ask Question\", variant=\"primary\", size=\"lg\")\n",
    "                        clear_btn = gr.Button(\"🧹 Clear Memory\", variant=\"secondary\")\n",
    "\n",
    "                with gr.Column(scale=3):\n",
    "                    response_text = gr.Textbox(\n",
    "                        label=\"🤖 AI Response\",\n",
    "                        lines=12,\n",
    "                        max_lines=25\n",
    "                    )\n",
    "\n",
    "                    response_audio = gr.Audio(\n",
    "                        label=\"🔊 Listen to Response\",\n",
    "                        type=\"filepath\"\n",
    "                    )\n",
    "\n",
    "                    last_question = gr.Textbox(\n",
    "                        label=\"❓ Last Question\",\n",
    "                        lines=1,\n",
    "                        interactive=False\n",
    "                    )\n",
    "\n",
    "        with gr.Tab(\"📊 Conversation History\"):\n",
    "            with gr.Column():\n",
    "                refresh_history_btn = gr.Button(\"🔄 Refresh History\", variant=\"secondary\")\n",
    "                conversation_history = gr.Textbox(\n",
    "                    label=\"📜 Recent Conversations\",\n",
    "                    lines=15,\n",
    "                    max_lines=25,\n",
    "                    interactive=False\n",
    "                )\n",
    "\n",
    "        with gr.Tab(\"⚙️ System Info\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ## 🧩 System Architecture\n",
    "\n",
    "\n",
    "            ### Configuration\n",
    "            - **Model**: Mistral Large Latest\n",
    "            - **Embedding**: all-MiniLM-L6-v2\n",
    "            - **Whisper**: base\n",
    "            - **Vector DB**: ChromaDB\n",
    "            - **LangSmith Project**: final3\n",
    "            \"\"\")\n",
    "\n",
    "        # Event handlers\n",
    "        ask_btn.click(\n",
    "            fn=process_question,\n",
    "            inputs=[text_input, audio_input],\n",
    "            outputs=[response_text, response_audio, last_question]\n",
    "        )\n",
    "\n",
    "        clear_btn.click(\n",
    "            fn=lambda: (rag_agent.clear_memory(), \"Memory cleared! 🧹\"),\n",
    "            inputs=[],\n",
    "            outputs=[conversation_history]\n",
    "        )\n",
    "\n",
    "        refresh_history_btn.click(\n",
    "            fn=get_conversation_history,\n",
    "            inputs=[],\n",
    "            outputs=[conversation_history]\n",
    "        )\n",
    "\n",
    "        # Example questions\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"Is AI dangerous?\"],\n",
    "                [\"How much caffeine per day is recommended?\"],\n",
    "                [\"What is Stress?\"]\n",
    "            ],\n",
    "            inputs=text_input\n",
    "        )\n",
    "\n",
    "    return interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89bd7697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:🚀 Initializing YouTube RAG Bot...\n",
      "INFO:root:✅ Found 30 audio files, skipping download\n",
      "INFO:root:🧠 Loading Whisper model...\n",
      "INFO:root:✅ Processed 30 transcripts\n",
      "WARNING:tensorflow:From C:\\Users\\rospi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "INFO:datasets:PyTorch version 2.7.1 available.\n",
      "INFO:datasets:TensorFlow version 2.19.0 available.\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:root:✅ Loading existing vectorstore...\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:root:✅ System initialized successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://localhost:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: what is stress?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Anxiety Changes You More Than You Think!**\n",
      "Have you ever been worried sick about something that felt so anxious that it made your body feel different? Anxiety is your body's natural response to stress. But when it lingers too long, it can actually cause real. Lasting changes, some more permanent than you might expect. I realize I might be making you anxious with how I'm talking. Let me try a friendlier tone. Hey there, welcome to Life Nogg...\n",
      "\n",
      "**Source 2: Anxiety Changes You More Than You Think!**\n",
      ". Constant muscle tension can lead to chronic pain, migraines and even changes in posture that put strain on the spine and joints. Anxiety's impact on digestion can cause irritable bowel syndrome or worsen acid reflux, leading to persistent stomach discomfort. High stress levels don't just cause breakouts. They can also slow wound healing, thin hair, and contribute to conditions like eczema or pso...\n",
      "\n",
      "**Source 3: Don't Watch Unless You're VampireDragon9925!**\n",
      ". See? DearBlocko, how do you make a smoothie? By the way, you blend fruit, try to add some greens like spinach and kale, but massage the kale first. Are these all from the same Vampire Dragon? Wait, these are all from the last six months and we never answered any of them? Oh man, now I feel bad. Uh, well they're all here, so I guess this is a DearBlocko speed round. It's your lucky day Vampire Dr...\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:🧹 Memory cleared\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: what is stress?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Anxiety Changes You More Than You Think!**\n",
      "Have you ever been worried sick about something that felt so anxious that it made your body feel different? Anxiety is your body's natural response to stress. But when it lingers too long, it can actually cause real. Lasting changes, some more permanent than you might expect. I realize I might be making you anxious with how I'm talking. Let me try a friendlier tone. Hey there, welcome to Life Nogg...\n",
      "\n",
      "**Source 2: Anxiety Changes You More Than You Think!**\n",
      ". Constant muscle tension can lead to chronic pain, migraines and even changes in posture that put strain on the spine and joints. Anxiety's impact on digestion can cause irritable bowel syndrome or worsen acid reflux, leading to persistent stomach discomfort. High stress levels don't just cause breakouts. They can also slow wound healing, thin hair, and contribute to conditions like eczema or pso...\n",
      "\n",
      "**Source 3: Don't Watch Unless You're VampireDragon9925!**\n",
      ". See? DearBlocko, how do you make a smoothie? By the way, you blend fruit, try to add some greens like spinach and kale, but massage the kale first. Are these all from the same Vampire Dragon? Wait, these are all from the last six months and we never answered any of them? Oh man, now I feel bad. Uh, well they're all here, so I guess this is a DearBlocko speed round. It's your lucky day Vampire Dr...\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: what is stress?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Anxiety Changes You More Than You Think!**\n",
      "Have you ever been worried sick about something that felt so anxious that it made your body feel different? Anxiety is your body's natural response to stress. But when it lingers too long, it can actually cause real. Lasting changes, some more permanent than you might expect. I realize I might be making you anxious with how I'm talking. Let me try a friendlier tone. Hey there, welcome to Life Nogg...\n",
      "\n",
      "**Source 2: Anxiety Changes You More Than You Think!**\n",
      ". Constant muscle tension can lead to chronic pain, migraines and even changes in posture that put strain on the spine and joints. Anxiety's impact on digestion can cause irritable bowel syndrome or worsen acid reflux, leading to persistent stomach discomfort. High stress levels don't just cause breakouts. They can also slow wound healing, thin hair, and contribute to conditions like eczema or pso...\n",
      "\n",
      "**Source 3: Don't Watch Unless You're VampireDragon9925!**\n",
      ". See? DearBlocko, how do you make a smoothie? By the way, you blend fruit, try to add some greens like spinach and kale, but massage the kale first. Are these all from the same Vampire Dragon? Wait, these are all from the last six months and we never answered any of them? Oh man, now I feel bad. Uh, well they're all here, so I guess this is a DearBlocko speed round. It's your lucky day Vampire Dr...\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:🧹 Memory cleared\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: what is stress?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Anxiety Changes You More Than You Think!**\n",
      "Have you ever been worried sick about something that felt so anxious that it made your body feel different? Anxiety is your body's natural response to stress. But when it lingers too long, it can actually cause real. Lasting changes, some more permanent than you might expect. I realize I might be making you anxious with how I'm talking. Let me try a friendlier tone. Hey there, welcome to Life Nogg...\n",
      "\n",
      "**Source 2: Anxiety Changes You More Than You Think!**\n",
      ". Constant muscle tension can lead to chronic pain, migraines and even changes in posture that put strain on the spine and joints. Anxiety's impact on digestion can cause irritable bowel syndrome or worsen acid reflux, leading to persistent stomach discomfort. High stress levels don't just cause breakouts. They can also slow wound healing, thin hair, and contribute to conditions like eczema or pso...\n",
      "\n",
      "**Source 3: Don't Watch Unless You're VampireDragon9925!**\n",
      ". See? DearBlocko, how do you make a smoothie? By the way, you blend fruit, try to add some greens like spinach and kale, but massage the kale first. Are these all from the same Vampire Dragon? Wait, these are all from the last six months and we never answered any of them? Oh man, now I feel bad. Uh, well they're all here, so I guess this is a DearBlocko speed round. It's your lucky day Vampire Dr...\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Stress is your body's natural response to anxiety or demanding situations. It can lead to various physical changes and health issues if it persists for too long. For instance, constant muscle tension from stress can result in chronic pain, migraines, and posture changes that strain the spine and joints. High stress levels can also affect digestion, causing issues like irritable bowel syndrome or worsening acid reflux, and can contribute to skin conditions like eczema or psoriasis. This information is based on the video titled \"Anxiety Changes You More Than You Think!\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: How much coffee per day?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Are You Having Too Much Caffeine？ ☕⚠️**\n",
      "Hey there, welcome to Life Noggin. I'm not a gambler, but I bet a lot of pizza-bacos that you've taken the most widely used psychoactive substance in the world, caffeine. Want to guess how many cups I've had today? Too slow, it was eight. Over 90% of adults living in the US use caffeine regularly consuming around 200 milligrams each day. The FDA says to save to have up to 400 milligrams a day. Wha...\n",
      "\n",
      "**Source 2: Are You Having Too Much Caffeine？ ☕⚠️**\n",
      ". But the FDA estimates that less than a half a teaspoon, which equals 1,200 milligrams, is enough to cause toxic effects like seizures. And I crashed. Oh, my head. I think I'll just stick with gorging on pizza bagels. So how many cups of coffee do you drink a day? Do you think you're addicted? Be honest. Let me hear it in the comments section below. Click here to watch this video on something. I ...\n",
      "\n",
      "**Source 3: Are You Having Too Much Caffeine？ ☕⚠️**\n",
      ". Woo! But there are some products with loads more, and consuming these could be really dangerous. I'd never do that, I'm not addicted, I don't have a problem. Caffeine stimulates the brain specifically the central nervous system. This is the part responsible for receiving, processing, and responding to sensory information. Caffeine makes you feel more awake and alert. It can also improve your moo...\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Based on the video titled \"Are You Having Too Much Caffeine？ ☕⚠️\", it is mentioned that the FDA recommends having up to 400 milligrams of caffeine a day. This is equivalent to about four cups of brewed coffee. However, consuming less than half a teaspoon of caffeine, which equals 1,200 milligrams, can cause toxic effects like seizures. It's important to monitor your caffeine intake to avoid these risks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Mistral API error: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":null}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `❌ Error: Unable to process request`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: vector_search\n",
      "Action Input: when do we eat vegetables?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m**Source 1: Don't Watch Unless You're VampireDragon9925!**\n",
      ". We're talking about a fiber overload, people. Some fruits are also high in natural sugars, which can spike your blood sugar if you consume too much. But stick with the veggies and fruits over popcorn candy, please. DearBlocko, what happens if there's food stuck in my teeth? Short term, you get some laughs from a friend or significant other. Short term, that food can mix with bacteria and create ...\n",
      "\n",
      "**Source 2: Don't Watch Unless You're VampireDragon9925!**\n",
      ". Stinky and unsanitary, fat idea, moving on. DearBlocko, can eating too much candy make you sick? Most definitely, too much candy can upset your stomach because of all that sugar. This can spike your blood sugar and cause headaches and nausea. Eating too much sugar over time is also bad for your teeth and can lead to bigger health problems. DearBlocko is burnt popcorn safe to eat. Didn't you ask ...\n",
      "\n",
      "**Source 3: Don't Watch Unless You're VampireDragon9925!**\n",
      "Hey there, welcome to DearBlocko. Tryngobb, let's bring out the question bag and answer some viewer questions. First up, we have one from Vampire Dragon 9925. Cool name, DearBlocko, what happens if we accidentally eat spoiled food? Well, you might start vomiting or have diarrhea. Basically, your body is trying to shoot the bad stuff out ASAP. This can happen due to bacteria like salmonella or coli...\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "\n",
      "AI: Based on the video titled \"Don't Watch Unless You're VampireDragon9925!\", it is mentioned that vegetables are a good source of fiber and should be preferred over sugary snacks like candy. However, there is no specific information on when to eat vegetables. If you have more specific questions about incorporating vegetables into your diet, feel free to ask!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "# ==================== Main Execution ====================\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize the system\n",
    "        initialize_system()\n",
    "        \n",
    "        # Create and launch interface\n",
    "        app = create_gradio_interface()\n",
    "        app.launch(\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=7860,\n",
    "            share=False,  # Creates public link\n",
    "            debug=True\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to start application: {e}\")\n",
    "        print(f\"❌ Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
